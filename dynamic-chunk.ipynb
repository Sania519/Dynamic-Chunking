{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-11T02:07:41.287371Z",
     "iopub.status.busy": "2025-12-11T02:07:41.287051Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers faiss-cpu transformers torch accelerate safetensors psutil\n",
    "!pip install faiss-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Faiss required (faiss-cpu or faiss-gpu). Install with pip. \" + str(e))\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "except Exception as e:\n",
    "    raise ImportError(\"torch + transformers required. Install with pip. \" + str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "EMBED_DIM = 384                    \n",
    "MISTRAL_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "MAX_CHARS_PER_CHUNK = 1500\n",
    "MIN_CHARS_PER_CHUNK = 400\n",
    "MAX_CHARS_PER_SUBUNIT = 700        \n",
    "FAISS_TARGET_NLIST = 256\n",
    "FAISS_USE_GPU_IF_AVAILABLE = True\n",
    "RAG_MAX_CONTEXT_CHARS = 8000\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Unit:\n",
    "    type: str                 \n",
    "    text: str\n",
    "    level: Optional[int] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    doc_id: str\n",
    "    chunk_id: str\n",
    "    section_path: List[str]\n",
    "    units: List[Unit] = field(default_factory=list)\n",
    "    char_len: int = 0\n",
    "    position: int = 0\n",
    "    embedding: Optional[np.ndarray] = None  \n",
    "\n",
    "    @property\n",
    "    def text(self) -> str:\n",
    "        return \"\\n\\n\".join(u.text for u in self.units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "HEADING_RE_NUM = re.compile(r\"^\\s*\\d+(\\.\\d+)*\\s+\")\n",
    "HEADING_RE_HASH = re.compile(r\"^\\s*#{1,6}\\s+\")\n",
    "HEADING_RE_ALLCAPS = re.compile(r\"^[A-Z0-9 ,;:\\-]{8,}$\")\n",
    "SENTENCE_SPLIT_RE = re.compile(r'(?<=[.!?])\\s+')\n",
    "\n",
    "\n",
    "def detect_heading_level(line: str) -> Optional[int]:\n",
    "    if HEADING_RE_HASH.match(line):\n",
    "        m = re.match(r\"^\\s*(#+)\", line)\n",
    "        if m:\n",
    "            return len(m.group(1))\n",
    "    if HEADING_RE_NUM.match(line):\n",
    "        num_part = line.strip().split()[0]\n",
    "        return num_part.count(\".\") + 1\n",
    "    if HEADING_RE_ALLCAPS.match(line.strip()):\n",
    "        return 2\n",
    "    return None\n",
    "\n",
    "\n",
    "def segment_into_units(text: str) -> List[Unit]:\n",
    "    units: List[Unit] = []\n",
    "    current_para: List[str] = []\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    def flush_para():\n",
    "        nonlocal current_para\n",
    "        if current_para:\n",
    "            txt = \" \".join(l.strip() for l in current_para).strip()\n",
    "            if txt:\n",
    "                units.append(Unit(type=\"paragraph\", text=txt))\n",
    "            current_para = []\n",
    "\n",
    "    for raw in lines:\n",
    "        line = raw.rstrip(\"\\n\")\n",
    "        if not line.strip():\n",
    "            flush_para()\n",
    "            continue\n",
    "\n",
    "        if re.match(r\"^\\s*[-*â€¢]\\s+\", line) or re.match(r\"^\\s*\\d+\\.\\s+\", line):\n",
    "            flush_para()\n",
    "            units.append(Unit(type=\"list\", text=line.strip()))\n",
    "            continue\n",
    "\n",
    "        level = detect_heading_level(line.strip())\n",
    "        if level is not None:\n",
    "            flush_para()\n",
    "            units.append(Unit(type=\"heading\", text=line.strip(), level=level))\n",
    "            continue\n",
    "\n",
    "        current_para.append(line)\n",
    "\n",
    "    flush_para()\n",
    "    return units\n",
    "\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    parts = SENTENCE_SPLIT_RE.split(text.strip())\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "\n",
    "def split_long_paragraph_unit(unit: Unit, max_chars_per_subunit: int) -> List[Unit]:\n",
    "    if unit.type != \"paragraph\" or len(unit.text) <= max_chars_per_subunit:\n",
    "        return [unit]\n",
    "\n",
    "    sentences = split_into_sentences(unit.text)\n",
    "    out: List[Unit] = []\n",
    "    cur_sentences: List[str] = []\n",
    "    cur_len = 0\n",
    "    for s in sentences:\n",
    "        sl = len(s)\n",
    "        if cur_sentences and (cur_len + 1 + sl > max_chars_per_subunit):\n",
    "            out.append(Unit(type=\"paragraph\", text=\" \".join(cur_sentences).strip()))\n",
    "            cur_sentences = []\n",
    "            cur_len = 0\n",
    "        cur_sentences.append(s)\n",
    "        cur_len += sl + 1\n",
    "    if cur_sentences:\n",
    "        out.append(Unit(type=\"paragraph\", text=\" \".join(cur_sentences).strip()))\n",
    "    return out\n",
    "\n",
    "\n",
    "def normalize_units_by_sentence(units: List[Unit], max_chars_per_subunit: int) -> List[Unit]:\n",
    "    out: List[Unit] = []\n",
    "    for u in units:\n",
    "        if u.type == \"paragraph\" and len(u.text) > max_chars_per_subunit:\n",
    "            out.extend(split_long_paragraph_unit(u, max_chars_per_subunit))\n",
    "        else:\n",
    "            out.append(u)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def update_section_path(section_path: List[str], level: int, heading_text: str) -> List[str]:\n",
    "    new = section_path[: level - 1]\n",
    "    new.append(heading_text)\n",
    "    return new\n",
    "\n",
    "\n",
    "def chunk_document(\n",
    "    units: List[Unit],\n",
    "    doc_id: str,\n",
    "    max_chars_per_chunk: int = MAX_CHARS_PER_CHUNK,\n",
    "    min_chars_per_chunk: int = MIN_CHARS_PER_CHUNK,\n",
    ") -> List[Chunk]:\n",
    "    chunks: List[Chunk] = []\n",
    "    section_path: List[str] = []\n",
    "\n",
    "    def new_chunk(pos: int) -> Chunk:\n",
    "        return Chunk(doc_id=doc_id, chunk_id=f\"{doc_id}_chunk_{pos}\", section_path=list(section_path), position=pos)\n",
    "\n",
    "    current_chunk = new_chunk(0)\n",
    "\n",
    "    for unit in units:\n",
    "        if unit.type == \"heading\" and unit.level is not None:\n",
    "            if current_chunk.units:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = new_chunk(len(chunks))\n",
    "            section_path = update_section_path(section_path, unit.level, unit.text)\n",
    "            continue\n",
    "\n",
    "        unit_len = len(unit.text)\n",
    "        if current_chunk.char_len + unit_len > max_chars_per_chunk:\n",
    "            if not current_chunk.units:\n",
    "                current_chunk.units.append(unit)\n",
    "                current_chunk.char_len += unit_len\n",
    "                continue\n",
    "            if current_chunk.char_len >= min_chars_per_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = new_chunk(len(chunks))\n",
    "                current_chunk.units.append(unit)\n",
    "                current_chunk.char_len += unit_len\n",
    "                continue\n",
    "            current_chunk.units.append(unit)\n",
    "            current_chunk.char_len += unit_len\n",
    "            continue\n",
    "\n",
    "        current_chunk.units.append(unit)\n",
    "        current_chunk.char_len += unit_len\n",
    "\n",
    "    if current_chunk.units:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Embedder:\n",
    "    def __init__(self, model_name: str = EMBED_MODEL_NAME, device: str = DEVICE):\n",
    "        print(f\"[embedder] Loading model {model_name} on {device} ...\")\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "\n",
    "    def encode_batch(self, texts: List[str], batch_size: int = 64) -> np.ndarray:\n",
    "        all_vecs = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i : i + batch_size]\n",
    "            enc = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():\n",
    "                out = self.model(**enc)\n",
    "            last = out.last_hidden_state  \n",
    "            mask = enc[\"attention_mask\"].unsqueeze(-1)\n",
    "            summed = (last * mask).sum(dim=1)\n",
    "            lens = mask.sum(dim=1)\n",
    "            vecs = (summed / lens).cpu().numpy()\n",
    "            norms = np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-10\n",
    "            vecs = vecs / norms\n",
    "            all_vecs.append(vecs.astype(\"float32\"))\n",
    "        return np.vstack(all_vecs)\n",
    "\n",
    "\n",
    "def embed_chunks(chunks: List[Chunk], embedder: Embedder, batch_size: int = 64):\n",
    "    texts = [c.text for c in chunks]\n",
    "    vecs = embedder.encode_batch(texts, batch_size=batch_size)\n",
    "    for i, c in enumerate(chunks):\n",
    "        c.embedding = vecs[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_faiss_index(chunks: List[Chunk], use_gpu: bool = FAISS_USE_GPU_IF_AVAILABLE, target_nlist: int = FAISS_TARGET_NLIST):\n",
    "    vectors = np.vstack([c.embedding for c in chunks]).astype(\"float32\")\n",
    "    N, dim = vectors.shape\n",
    "    print(f\"[faiss] N={N}, dim={dim}\")\n",
    "\n",
    "    if N == 0:\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        return index, {}\n",
    "\n",
    "    faiss.normalize_L2(vectors)\n",
    "\n",
    "    if N < 64:\n",
    "        print(\"[faiss] Too few vectors -> using IndexFlatIP (no train)\")\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        if use_gpu and faiss.get_num_gpus() > 0:\n",
    "            res = faiss.StandardGpuResources()\n",
    "            index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "        index.add(vectors)\n",
    "        label_map = {i: chunks[i] for i in range(N)}\n",
    "        return index, label_map\n",
    "\n",
    "    nlist = min(target_nlist, max(8, N // 10))\n",
    "    print(f\"[faiss] Using IVF with nlist={nlist} (target {target_nlist})\")\n",
    "    quantizer = faiss.IndexFlatIP(dim)\n",
    "    index = faiss.IndexIVFFlat(quantizer, dim, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "    train_size = max(nlist, min(N, 50000))\n",
    "    idx = np.random.choice(N, train_size, replace=False)\n",
    "    train_sample = vectors[idx]\n",
    "    print(f\"[faiss] Training with {train_size} samples...\")\n",
    "    index.train(train_sample)\n",
    "\n",
    "    print(\"[faiss] Adding vectors...\")\n",
    "    index.add(vectors)\n",
    "    print(f\"[faiss] ntotal={index.ntotal}\")\n",
    "\n",
    "    if use_gpu and faiss.get_num_gpus() > 0:\n",
    "        print(\"[faiss] Moving index to GPU (single GPU)...\")\n",
    "        res = faiss.StandardGpuResources()\n",
    "        co = faiss.GpuClonerOptions()\n",
    "        co.useFloat16 = True\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, index, co)\n",
    "\n",
    "    try:\n",
    "        index.nprobe = max(1, min(FAISS_TARGET_NLIST, int(math.sqrt(nlist))))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    label_map = {i: chunks[i] for i in range(N)}\n",
    "    return index, label_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(question: str, index, label_map: Dict[int, Chunk], top_k: int = 6):\n",
    "    if index is None or index.ntotal == 0:\n",
    "        return []\n",
    "\n",
    "    q_vec = embedder.encode_batch([question], batch_size=1)\n",
    "    faiss.normalize_L2(q_vec)\n",
    "    k = min(top_k, index.ntotal)\n",
    "    D, I = index.search(q_vec, k)\n",
    "    D = D[0]\n",
    "    I = I[0]\n",
    "    out = []\n",
    "    for idx, score in zip(I, D):\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        out.append((label_map[int(idx)], float(score)))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "_mistral_model = None\n",
    "_mistral_tokenizer = None\n",
    "\n",
    "\n",
    "def load_mistral(model_name: str = MISTRAL_MODEL):\n",
    "    global _mistral_model, _mistral_tokenizer\n",
    "    if _mistral_model is not None:\n",
    "        return _mistral_model, _mistral_tokenizer\n",
    "    print(f\"[mistral] loading {model_name} on {DEVICE} ...\")\n",
    "    _mistral_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    _mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "    )\n",
    "    return _mistral_model, _mistral_tokenizer\n",
    "\n",
    "\n",
    "def call_mistral(system_prompt: str, user_prompt: str, max_new_tokens: int = 512):\n",
    "    model, tokenizer = load_mistral()\n",
    "    full = f\"<s>[INST] {system_prompt}\\n\\n{user_prompt} [/INST]\"\n",
    "    inputs = tokenizer(full, return_tensors=\"pt\", truncation=True, max_length=4096).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    gen = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    if \"[/INST]\" in gen:\n",
    "        gen = gen.split(\"[/INST]\", 1)[1].strip()\n",
    "    return gen.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_answer_rag(question: str, retrieved: List[Tuple[Chunk, float]], max_context_chars: int = RAG_MAX_CONTEXT_CHARS):\n",
    "    parts = []\n",
    "    used = 0\n",
    "    for chunk, score in retrieved:\n",
    "        block = f\"[Doc: {chunk.doc_id} | Chunk: {chunk.chunk_id} | Sec: {' > '.join(chunk.section_path)} | Score: {score:.4f}]\\n{chunk.text}\\n\"\n",
    "        if used + len(block) > max_context_chars:\n",
    "            break\n",
    "        parts.append(block)\n",
    "        used += len(block)\n",
    "    context = \"\\n\\n\".join(parts)\n",
    "    system_prompt = \"You are a helpful assistant. Answer using ONLY the provided excerpts. If not supported, say you don't know.\"\n",
    "    user_prompt = f\"Question:\\n{question}\\n\\nExcerpts:\\n{context}\\n\\nAnswer:\"\n",
    "    return call_mistral(system_prompt, user_prompt)\n",
    "\n",
    "\n",
    "class RAGEngine:\n",
    "    def __init__(self, embedder: Embedder):\n",
    "        self.chunks: List[Chunk] = []\n",
    "        self.index = None\n",
    "        self.label_map = {}\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def build_from_docs(self, docs: Dict[str, str],\n",
    "                        max_chars_per_chunk: int = MAX_CHARS_PER_CHUNK,\n",
    "                        min_chars_per_chunk: int = MIN_CHARS_PER_CHUNK,\n",
    "                        max_chars_per_subunit: int = MAX_CHARS_PER_SUBUNIT):\n",
    "        all_chunks: List[Chunk] = []\n",
    "        for doc_id, text in docs.items():\n",
    "            units = segment_into_units(text)\n",
    "            units = normalize_units_by_sentence(units, max_chars_per_subunit)\n",
    "            doc_chunks = chunk_document(units, doc_id, max_chars_per_chunk, min_chars_per_chunk)\n",
    "            all_chunks.extend(doc_chunks)\n",
    "\n",
    "        print(f\"[rag] Created {len(all_chunks)} chunks from {len(docs)} docs.\")\n",
    "        self.chunks = all_chunks\n",
    "\n",
    "        embed_chunks(self.chunks, self.embedder, batch_size=64)\n",
    "\n",
    "        self.index, self.label_map = build_faiss_index(self.chunks, use_gpu=FAISS_USE_GPU_IF_AVAILABLE, target_nlist=FAISS_TARGET_NLIST)\n",
    "\n",
    "    def answer(self, question: str, top_k: int = 6) -> str:\n",
    "        if self.index is None:\n",
    "            raise RuntimeError(\"Index not built\")\n",
    "        retrieved = retrieve_relevant_chunks(question, self.index, self.label_map, top_k=top_k)\n",
    "        return generate_answer_rag(question, retrieved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mem_mb():\n",
    "    return psutil.Process().memory_info().rss / (1024**2)\n",
    "\n",
    "\n",
    "def synthetic_doc(size_chars: int) -> str:\n",
    "    block = (\n",
    "        \"This is a synthetic sentence used for stress-testing the dynamic chunking \"\n",
    "        \"and FAISS indexing pipeline. It contains a moderate number of characters \"\n",
    "        \"to simulate realistic text distribution. \"\n",
    "    )\n",
    "    out = []\n",
    "    while sum(len(x) for x in out) < size_chars:\n",
    "        out.append(block)\n",
    "    return \"\".join(out)[:size_chars]\n",
    "\n",
    "\n",
    "def stress_test(size_chars: int):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"STRESS TEST: {size_chars:,} chars\")\n",
    "    print(\"=\"*70)\n",
    "    start_mem = mem_mb()\n",
    "    t0 = time.time()\n",
    "    doc = synthetic_doc(size_chars)\n",
    "    t1 = time.time()\n",
    "    print(f\"Generated doc in {t1-t0:.2f}s, mem delta: {mem_mb()-start_mem:.2f} MB\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    units = segment_into_units(doc)\n",
    "    units = normalize_units_by_sentence(units, MAX_CHARS_PER_SUBUNIT)\n",
    "    t1 = time.time()\n",
    "    print(f\"Segmented -> units: {len(units)}, time: {t1-t0:.2f}s, mem delta: {mem_mb()-start_mem:.2f} MB\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    chunks = chunk_document(units, doc_id=\"stress\", max_chars_per_chunk=MAX_CHARS_PER_CHUNK, min_chars_per_chunk=MIN_CHARS_PER_CHUNK)\n",
    "    t1 = time.time()\n",
    "    print(f\"Chunked -> chunks: {len(chunks)}, avg chars/chunk: {np.mean([c.char_len for c in chunks]):.1f}, time: {t1-t0:.2f}s, mem delta: {mem_mb()-start_mem:.2f} MB\")\n",
    "\n",
    "    print(\"[stress] Instantiating embedder for stress embedding (may be slow).\")\n",
    "    ed = Embedder()\n",
    "    t0 = time.time()\n",
    "    embed_chunks(chunks, ed, batch_size=128)\n",
    "    t1 = time.time()\n",
    "    print(f\"Embedded {len(chunks)} chunks in {t1-t0:.2f}s, mem delta: {mem_mb()-start_mem:.2f} MB\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        idx, lbl_map = build_faiss_index(chunks, use_gpu=FAISS_USE_GPU_IF_AVAILABLE, target_nlist=FAISS_TARGET_NLIST)\n",
    "        t1 = time.time()\n",
    "        print(f\"Built FAISS index in {t1-t0:.2f}s, mem delta: {mem_mb()-start_mem:.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(\"FAISS build failed:\", e)\n",
    "        return\n",
    "\n",
    "    q = \"What does this synthetic document describe?\"\n",
    "    t0 = time.time()\n",
    "    results = retrieve_relevant_chunks(q, idx, lbl_map, top_k=6)\n",
    "    t1 = time.time()\n",
    "    print(f\"Retrieved {len(results)} chunks in {t1-t0:.2f}s, mem delta: {mem_mb()-start_mem:.2f} MB\")\n",
    "    print(\"Stress test done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedder = Embedder()\n",
    "rag = RAGEngine(embedder)\n",
    "docs = {\n",
    "    \"doc_1\": \"\"\"\n",
    "1. Introduction\n",
    "This document describes the system we are building.\n",
    "\n",
    "1.1 Background\n",
    "The system handles streaming data from multiple sources.\n",
    "We normalize and validate inputs before processing.\n",
    "\n",
    "1.2 Missing Data\n",
    "If values are missing, we apply simple imputation strategies\n",
    "such as forward fill or mean imputation depending on the feature.\n",
    "\n",
    "2. Methods\n",
    "We use a transformer-based model with attention over time windows.\n",
    "\"\"\",\n",
    "    \"doc_2\": \"\"\"\n",
    "1. Overview\n",
    "This document explains retry and backoff strategy.\n",
    "\n",
    "2. Retries\n",
    "We retry failed requests with exponential backoff,\n",
    "capped at a maximum delay and limited number of attempts.\n",
    "\n",
    "3. Circuit Breaking\n",
    "If too many failures occur, the circuit opens and\n",
    "we temporarily stop sending requests to the downstream service.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(\"[demo] Building index from demo docs...\")\n",
    "rag.build_from_docs(docs)\n",
    "q = \"How does the system handle missing data?\"\n",
    "print(\"\\nQUESTION:\", q)\n",
    "ans = rag.answer(q, top_k=6)\n",
    "print(\"\\nANSWER:\\n\", ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sizes = [100_000, 1_000_000, 5_000_000, 10_000_000]\n",
    "for s in sizes:\n",
    "    stress_test(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
